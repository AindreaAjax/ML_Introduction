{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with Time Series Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "24ca51e2-cb7e-4663-b94d-c4ceb6840f46",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Import Statements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786e11f7",
   "metadata": {},
   "source": [
    "--------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b98e008-5cd4-4103-a41d-aa577f7b0d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import statements\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "62204132-8954-4462-b1b7-fbc807c3b906",
   "metadata": {},
   "outputs": [],
   "source": [
    "# view options\n",
    "pd.set_option(\"display.max_columns\", 40)\n",
    "pd.set_option(\"display.max_rows\", 6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "857d3177-ade7-4dc9-beef-fb0674bf2e30",
   "metadata": {
    "tags": []
   },
   "source": [
    "---------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Time Data and Handling Timezone Information"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Basic informations and some knowhows about Timezones*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Coordinated Universal Time (UTC) is the time standard at 0 degrees longitude. It has an excellent property, that it is monotonically increasing. For example, Salt Lake City, Utah is in the America/Denver timezone, which is 6 or 7 hours offset of UTC depending on the time of\n",
    "year (due to Day light saving). Thus we see, a timezone may contain one or more offsets.\n",
    "\n",
    "Some terminologies: A time without a timezone or offset is called ”naive” time. A time specified in local time is also called ”civil time” or ”wall time”.\n",
    "\n",
    "Timezones that have daylight savings time can have ”ambiguous time” in the fall when the time goes back. For this reason, if you are dealing with **local times**, you will want three things: **the time, the timezone, and an offset**. If you are only concerned with **duration**, you can just use **UTC time** or seconds since **UNIX epoch**.\n",
    "\n",
    "A general recommendation for programmers is to store dates in UTC times and then convert them to local time as needed.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Getting the correct timezone name is important. The recommendation is prefacing your search with ”IANA” (ie. ”IANA Timezone for Salt Lake City”) and then double checking your result in this Wikipedia article (https://en.wikipedia.org/wiki/List_of_tz_database_time_zones)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Working with UTC time data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "utc_data = pd.read_csv(\"./utc_time_data.csv\").UTC_Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2015 -03 -08 08:00:00-07:00\n",
       "1    2015 -03 -08 08:30:00-07:00\n",
       "2    2015 -03 -08 09:00:00+06:00\n",
       "Name: UTC_Date, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utc_data.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To convert a series containing utc date into a datetime64 object we can use the `pd.to_datetime(args, utc)` function and we need to set, **utc=True**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "utc_time = pd.to_datetime(utc_data, utc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11   2015-11-01 16:00:00+00:00\n",
       "12   2015-11-01 16:30:00+00:00\n",
       "13   2015-11-01 17:00:00+00:00\n",
       "Name: UTC_Date, dtype: datetime64[ns, UTC]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "utc_time.tail(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** It is `not necessary` that the UTC data has an `offset of 00:00`. Setting, `utc=True` will `automatically convert` the data to an `offset of 00:00`. If we need, we can change it using the .dt.tz_convert(timezone) method."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have converted a series to a datetime object we can utilize the `.dt` accessor, which gives us some awesome tools for dealing with dates."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To convert UTC datetime data to a certain timezone we can use the `.dt.tz_convert(timezone)` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9   2015-11-01 21:00:00+06:00\n",
       "2   2015-03-08 09:00:00+06:00\n",
       "5   2015-11-01 07:00:00+06:00\n",
       "Name: UTC_Date, dtype: datetime64[ns, Asia/Dhaka]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# say, we wanted to convert the utc_time to the Dhaka timezone (capital of Bangladesh)\n",
    "# from internet we determine that the correct timezone for Dhaka is, 'Asia/Dhaka'\n",
    "utc_time.dt.tz_convert(\"Asia/Dhaka\").sample(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Working with Local time data*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To load local date information, we need to have the date, the offset, and the timezone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "local = pd.read_csv(\"./local_time_data.csv\")\n",
    "local_date = local.local_date\n",
    "offset = local.offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12    2015 -11 -01 02:00:00\n",
       "14    2015 -11 -01 01:00:00\n",
       "3     2015 -03 -08 02:30:00\n",
       "Name: local_date, dtype: object"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_date.sample(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>**Workflow:**</u>\n",
    "1. First, we will convert the local_date to a naive (i.e, w/o timezone information) datetime format using the `pd.to_datetime()` function.\n",
    "2. We will group the local_date datetime object by the offset. If we pass in a series to a `groupby` object it will first align the passed series to the original data (will enter NaN if series length is smaller) [much like adding a new column to the data]. After that the actual grouping will be done. The groups will have the series value (that it was grouped by) as the name (accessible by `series.name`).\n",
    "3. If ordinary agg func is used the grouped data will have group index. To retain the original index positions we will be using the groupby `.transform()` method. \n",
    "4. After that, the naive data will be converted to a `timezone-aware` format with the help of, `.dt.tz_localize(tz:str, pytz.timezone)` method.\n",
    "5. Finally we will be converting the timezone-aware object to the local timezone using the `.dt.tz_convert()` method."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One caveat in this process is that, our offset series doesn't have the proper timezone format i,e, `HH:MM`. As a result if we tried to produce a timezone aware datetime object it will only modify the MM and not the HH. So, first we convert the offset series entries to proper format. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see that, offset doesn't have the proper formatting\n",
    "# offset.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# formatting offset series entries to HH:MM\n",
    "offset = offset.replace({-7: \"-07:00\", -6: \"-06:00\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see what changed after formatting\n",
    "# offset.sample(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The actual work (we will convert the time to America/Denver timezone)\n",
    "local_time = pd.to_datetime(local_date).groupby(offset)\\\n",
    "            .transform(lambda s: s.dt.tz_localize(s.name).dt.tz_convert(\"America/Denver\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   2015-03-08 01:00:00-07:00\n",
       "1   2015-03-08 01:30:00-07:00\n",
       "2   2015-03-08 03:00:00-06:00\n",
       "Name: local_date, dtype: datetime64[ns, America/Denver]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_time.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.to_datetime(local_date).groupby(offset).transform(lambda s: print(s.name))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-Converting local time to UTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0   2015-03-08 08:00:00+00:00\n",
       "1   2015-03-08 08:30:00+00:00\n",
       "2   2015-03-08 09:00:00+00:00\n",
       "Name: local_date, dtype: datetime64[ns, UTC]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "local_time.dt.tz_convert(\"UTC\").head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Working with UNIX Epoch time*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To find the time elapsed (in seconds) since UNIX epoch at Jan 1, 1970 midnight UTC -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1425801600\n",
       "1    1425803400\n",
       "2    1425805200\n",
       "Name: local_date, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unix_local = local_time.view(int).floordiv(1e9).astype(int)\n",
    "unix_local.head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To convert epoch information into UTC -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2015-03-08 08:00:00+00:00\n",
       "1    2015-03-08 08:30:00+00:00\n",
       "2    2015-03-08 09:00:00+00:00\n",
       "                ...           \n",
       "16   2015-11-01 09:00:00+00:00\n",
       "17   2015-11-01 09:30:00+00:00\n",
       "18   2015-11-01 10:00:00+00:00\n",
       "Name: local_date, Length: 19, dtype: datetime64[ns, UTC]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.to_datetime(unix_local, unit='s').dt.tz_localize(\"UTC\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The `.dt` Accessor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-------------------------------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The .dt accessor provides us access to some very useful datatime properties and mehtods.\n",
    "\n",
    "DateTime properties: https://pandas.pydata.org/pandas-docs/stable/reference/series.html#datetime-properties\n",
    "\n",
    "DateTime methods: https://pandas.pydata.org/pandas-docs/stable/reference/series.html#datetime-methods\n",
    "\n",
    "Period properties: https://pandas.pydata.org/pandas-docs/stable/reference/series.html#period-properties \n",
    "\n",
    "TimeDelta properties: https://pandas.pydata.org/pandas-docs/stable/reference/series.html#timedelta-properties\n",
    "\n",
    "TimeDelta methods: https://pandas.pydata.org/pandas-docs/stable/reference/series.html#timedelta-methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>year</th>\n",
       "      <th>week</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2015</td>\n",
       "      <td>44</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2015</td>\n",
       "      <td>44</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2015</td>\n",
       "      <td>44</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    year  week  day\n",
       "15  2015    44    7\n",
       "11  2015    44    7\n",
       "10  2015    44    7"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For example to calculate year, week, and day according to the ISO 8601 standard\n",
    "local_time.dt.isocalendar().sample(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *The `dt.strftime()` method*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.dt.strftime(format)` method will convert a pandas datetime object to a string using the specified formatting style. To see all the available format codes see the documentation @https://docs.python.org/3/library/datetime.html#strftime-and-strptime-format-codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    08 Mar 2015 01:00 AM MST\n",
       "1    08 Mar 2015 01:30 AM MST\n",
       "2    08 Mar 2015 03:00 AM MDT\n",
       "Name: local_date, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for example, let's say we want our local_time series data as a string in the form of \n",
    "# e.g, 03 Jan 2000 02:44 PM MDT\n",
    "local_time.dt.strftime('%d %b %Y %I:%M %p %Z').head(3)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dates in the Index"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First let us read in the alta-noaa-1980-2019.csv dataset. This dataset contains information about the amount of snow fall in a ski resort."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "alta_df = pd.read_csv(\"./Data/alta-noaa-1980-2019.csv\", date_parser=\"DATE\").set_index(\"DATE\")\n",
    "alta_df.index = alta_df.index.astype(\"datetime64\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DATE\n",
       "1980-01-01    2.0\n",
       "1980-01-02    3.0\n",
       "1980-01-03    1.0\n",
       "             ... \n",
       "2019-09-05    0.0\n",
       "2019-09-06    0.0\n",
       "2019-09-07    0.0\n",
       "Name: snow, Length: 14160, dtype: float64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alta_df_snow = alta_df.SNOW.rename('snow')\n",
    "alta_df_snow"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Slicing Time series"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the index is a datetime object, we can slice it with a string (or a partial string) that represents the date. If we specify just the month on a slice, it includes all entries from that month (on both the start and end slices)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DATE\n",
       "1980-02-01     0.0\n",
       "1980-02-02     0.0\n",
       "1980-02-03     0.0\n",
       "              ... \n",
       "1980-03-29     0.0\n",
       "1980-03-30     0.0\n",
       "1980-03-31    13.0\n",
       "Name: snow, Length: 60, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alta_df_snow.loc[\"1980/2\":\"1980/3\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Finding missing data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# at first, let us check whether the data has any missing values at all\n",
    "alta_df_snow.isna().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DATE\n",
       "1985-07-30   NaN\n",
       "1985-09-12   NaN\n",
       "1985-09-19   NaN\n",
       "              ..\n",
       "2017-10-02   NaN\n",
       "2017-12-23   NaN\n",
       "2018-12-03   NaN\n",
       "Name: snow, Length: 365, dtype: float64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# looks like it indeed has missing values. let's create a filter to see which dates has missing values\n",
    "alta_df_snow.loc[alta_df_snow.isna()]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** The series object has no `.query` method. If it was a dataframe then we could have used the .query() method."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Handling Missing data*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**The best way to deal with missing data is to talk with a subject matter expert and determine why it is missing.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> <u>**Dropping missing values**</u> with the `dropna()` method"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Be careful with the method and only use it after talking to a subject matter expert who confirms that it is ok to drop the data. It can be hard to tell later if the data is missing. For example, if you plotted this data, you might not see that data was dropped unless you pay close attention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alta_df_snow.dropna().isna().any()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> <u>**Filling missing values**</u> with the help of `.fillna()`, `.ffill()`, `.bfill()` etc. Depending on the data contents, the `.mean()`, `.mode()`, `.median` and other such methods may come in handy while using the `.fillna()` method."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> <u>**Interpolating missing values**</u> with the `.interpolate()` method may also be appropriate in some cases. By default the interpolating method will be linear interpolation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> <u>**Using filling and interpolation in combination**</u> with the help of `.where()`/`.mask()` method"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As is often the case, the trend and characteristics of the data is such that handling missing values requires us to utilize different methods for different parts of the data. Our `alta_df_snow` data series is a good example of this. \n",
    "\n",
    "-> In winter (1st and 4th quarter of the year) we should use interpolate/ffill/bfill methods as, in winter it is most likey that there were snow in the days that are missing data.\n",
    "\n",
    "-> But, in summer (2nd and 3rd quarter of the year) we can assume that there were no snow and thus use fillna method to fill the missing values with 0.\n",
    "\n",
    "To find out which quarter a date (datetime object) falls on we can call the `.dt.quarter` property. In case of DateTimeIndex object we can directly call the `.quarter` property to serve the same purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "winter = (alta_df_snow.index.quarter == 1) | (alta_df_snow.index.quarter == 4)\n",
    "\n",
    "alta_df_snow.mask(winter & alta_df_snow.isna(), alta_df_snow.interpolate())\\\n",
    "            .mask(~winter & alta_df_snow.isna(), 0).isna().any()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Shifting Data*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.shift()` method works on any pandas series but comes in really useful with time series when we want to compare to the previous or subsequent entry."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Forward shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DATE\n",
       "1980-01-01    NaN\n",
       "1980-01-02    2.0\n",
       "1980-01-03    3.0\n",
       "             ... \n",
       "2019-09-05    0.0\n",
       "2019-09-06    0.0\n",
       "2019-09-07    0.0\n",
       "Name: snow, Length: 14160, dtype: float64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alta_df_snow.shift(1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Backward shift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DATE\n",
       "1980-01-01    3.0\n",
       "1980-01-02    1.0\n",
       "1980-01-03    0.0\n",
       "             ... \n",
       "2019-09-05    0.0\n",
       "2019-09-06    0.0\n",
       "2019-09-07    NaN\n",
       "Name: snow, Length: 14160, dtype: float64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alta_df_snow.shift(-1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Rolling window calculations*"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `.rolling()` function provides the feature of rolling window calculations. The concept of rolling window calculation is most primarily used in signal processing and time-series data. In very simple words we take a window size of k at a time and perform some desired mathematical operation on it. A window of size k means k consecutive values at a time. In a very simple case, all the ‘k’ values are equally weighted."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aggregate functions that works on the **rolling object** are, \n",
    "\n",
    "<img src='./aggregate_methods_that_work_on_rolling_objects.png'>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>**.rolling() function Parameters**</u>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-> window : Size of the moving window. This is the number of observations used for calculating the statistic. Each window will be a fixed size. If its an offset then this will be the time period of each window. Each window will be a variable sized based on the observations included in the time-period. This is only valid for datetimelike indexes. To learn more about the offsets & frequency strings, please see @https://pandas.pydata.org/pandas-docs/stable/user_guide/timeseries.html#offset-aliases.\n",
    "\n",
    "-> min_periods : Minimum number of observations in window required to have a value (otherwise result is NA). For a window that is specified by an offset, this will default to 1. \n",
    "\n",
    "-> win_type : If win_type=none, then all the values in the window are evenly weighted. There is various other types of rolling window type. To learn more about the other rolling window type see @https://docs.scipy.org/doc/scipy/reference/generated/scipy.signal.get_window.html#scipy.signal.get_window.\n",
    "\n",
    "-> on : For a DataFrame, column on which to calculate the rolling window, rather than the index."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Example: Calculating 3 day moving average snow fall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DATE\n",
       "1980-01-01    NaN\n",
       "1980-01-02    2.5\n",
       "1980-01-03    2.0\n",
       "             ... \n",
       "2019-09-05    0.0\n",
       "2019-09-06    0.0\n",
       "2019-09-07    0.0\n",
       "Name: snow, Length: 14160, dtype: float64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alta_df_snow.rolling(window=3, min_periods=2).mean()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resampling"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_intro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c68b33fb54bb59552268eca97cce0a66d93ba1e1c3c0870be37ac7a47ccc96a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
