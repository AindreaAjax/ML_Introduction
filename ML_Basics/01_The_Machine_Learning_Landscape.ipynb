{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Different types of Machine Learning](#toc1_)    \n",
    "  - [Supervised learning](#toc1_1_)    \n",
    "  - [Unsupervised learning](#toc1_2_)    \n",
    "  - [Semi-supervised learning](#toc1_3_)    \n",
    "  - [Self-supervised learning](#toc1_4_)    \n",
    "  - [Reinforcement learning](#toc1_5_)    \n",
    "- [Online learning and Batch learning](#toc2_)    \n",
    "- [Instance-based learning and Model-based learning](#toc3_)    \n",
    "- [Main challenges of Machine Learning](#toc4_)    \n",
    "- [Model evaluation, Hyperparameter tuning and Model selection](#toc5_)    \n",
    "  - [Testing and validating](#toc5_1_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=2\n",
    "\tmaxLevel=5\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc1_'></a>[Different types of Machine Learning](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Three of the most common ML models are, supervised, unsupervised and reinforcement learning (image source: ML with PyTorch and Scikit-Learn - Seb Raschka)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/the_three_different_types_of_machine_learning.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_1_'></a>[Supervised learning](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Supervised learning is used in scenarios where the data is already labeled and the goal is to train a model to learn the relationship between the input features and the target. The target is the label that we are trying to predict, such as whether an email is spam or not, or whether a tumor is malignant or benign, or house price prediction.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>**Naming conventions**</u>\n",
    "\n",
    "- **Features** are also known as predictors, inputs, attributes, independent variables etc.\n",
    "- **Target** is also known as response, outcome, label, dependent variable etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>**Different types of supervised learning problems**</u>\n",
    "\n",
    "- `Classification:` classification is a supervised learning task where the target variable is categorical, such as spam or not spam, or malignant or benign. The goal is to train a model to learn the relationship between the input features and the target which can then be used to predict the target variable for new unlabeled data.\n",
    "\n",
    "- `Regression:` regression is a supervised learning task where the target variable is continuous, such as house price or stock price prediction. \n",
    "\n",
    "- `Logistic regression:` in logistic regression, the dependent variable is a binary variable that contains data coded as 1 (yes, success, etc.) or 0 (no, failure, etc.). The logistic regression model predicts P(Y=1) as a function of X. It is often used for classification tasks, such as spam detection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_'></a>[Unsupervised learning](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In unsupervised learning, the data is unlabeled and the goal is to model the underlying structure or distribution in the data in order to learn more about the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<u>**Different types of unsupervised learning problems**</u>\n",
    "\n",
    "- `Clustering:` clustering is an unsupervised learning task where the goal is to group the data into distinct clusters such that observations within a cluster are very similar to each other, whereas observations in different clusters are very different from each other. Clustering is often used for exploratory analysis and/or as a component of a preprocessing pipeline in order to get the labels for a supervised learning task.\n",
    "\n",
    "- `Dimensionality reduction:` dimensionality reduction is an unsupervised learning task where the goal is to reduce the dimensionality (i.e., the number of features) of a data set by extracting a new set of features that preserves the most important information. It often does so by combining highly correlated features to form a smaller set of features that are more easily interpreted. Dimensionality reduction is often used as a component of a preprocessing pipeline.\n",
    "\n",
    "- `Association rule learning:` association rule learning is an unsupervised learning task where the goal is to discover rules that describe the relationship between variables in a data set. One example of association rule learning is market basket analysis. Market basket analysis is used to analyze customer behavior by finding associations between the different items that customers place in their “shopping baskets” while shopping at a supermarket or an online store. For example, a supermarket might discover that customers who buy butter and eggs also tend to buy bread, so they can put butter, eggs, and bread close to each other to increase sales.\n",
    "\n",
    "- `Anomaly detection:` anomaly detection (also outlier detection) is an unsupervised learning task where the goal is to identify observations that are significantly different from the rest of the data. For example, anomaly detection is used to detect fraud detection or defective items in manufacturing. The training data for anomaly detection is often highly imbalanced (i.e., most observations are not anomalies). As a result when the model sees a new observation it can output whether it is an anomaly or not. Anomaly detection is often used to automatically remove outliers from a data set before another model is applied.\n",
    "\n",
    "- `Novelty detection:` novelty detection is similar to anomaly detection except that novelty detection algorithms are expected to generalize better to new observations that are not part of the training data. For novelty detection model training, only normal data are used. The training set should not contain any instance that you would like to classify as a novelty. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsupervised learning can be a goal in itself, such as in clustering, or it can be used as a preprocessing step for supervised learning, such as in PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_3_'></a>[Semi-supervised learning](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In semi-supervised learning, the data is a mix of labeled and unlabeled examples. Most semi-supervised learning algorithms are combinations of unsupervised and supervised algorithms. For example, a clustering algorithm may be used to group similar instances together, and then every unlabeled instance can be labeled with the most common label in its cluster. Once the whole dataset is labeled, it is possible to use any supervised learning algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_4_'></a>[Self-supervised learning](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-supervised learning is a mix of supervised and unsupervised learning. First the model is trained in an unsupervised manner, typically to solve a task that can be automated (e.g., clustering, feature extraction, image colorization, predicting the missing part of an image, etc.). Then, the model is fine-tuned using supervised learning to perform the actual task of interest (e.g., classification). This makes it possible to train a good model using only a small amount of labeled training data, as long as there is a lot of unlabeled training data available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc1_5_'></a>[Reinforcement learning](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reinforcement learning is a very different beast. The learning system, called an agent in this context, can observe the environment, select and perform actions, and get rewards in return (or penalties in the form of negative rewards). It must then learn by itself what is the best strategy, called a policy, to get the most reward over time. A policy defines what action the agent should choose when it is in a given situation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc2_'></a>[Online learning and Batch learning](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Batch Learning:` Batch learning is also known as offline learning. In batch learning, the system is incapable of learning incrementally (i.e, the model can't be updated as new data comes in). Instead, it must be trained using all the available data. \n",
    "\n",
    "The advantage of batch learning is that it is generally simple to train (since you can automate the process) and the resulting model is usually accurate and stable (as launching the model is dependent on you). \n",
    "\n",
    "The downside is that the model needs to be retrained from scratch every time new data is available. This can be very time consuming and/or expensive, so it is typically done offline at regular intervals. Model retraining frequency depends on the model's performance decay rate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Online Learning:` In online learning, you train the system incrementally by feeding it data instances sequentially, either individually or by small groups called mini-batches. Each learning step is fast and cheap, so the system can learn about new data on the fly, as it arrives.\n",
    "\n",
    "One important parameter of online learning systems is how fast they should adapt to changing data: this is called the ***learning rate***. If you set a high learning rate, then your system will rapidly adapt to new data, but it will also tend to quickly forget the old data (and you don’t want a spam filter to flag only the latest kinds of spam it was shown). Conversely, if you set a low learning rate, the system will have more inertia; that is, it will learn more slowly, but it will also be less sensitive to noise in the new data or to sequences of nonrepresentative data points (outliers).\n",
    "\n",
    "The main challenges with online learning are that if bad data is fed to the model, the model's performance will decline, often quickly (depending on the data quality and learning rate). To deal with this it is advised to monitor the input data and deal with outliers (e.g., using anomaly detection algorithms). Also monitor the model's performance and if it drops, roll back to a previously working state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Online learning algorithms can be used to train models on huge datasets that cannot fit in one machine’s main memory, this is called ***out-of-core learning***. Out-of-core learning is usually done offline and not on the live system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc3_'></a>[Instance-based learning and Model-based learning](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Instance-based learning:` In instance-based learning, the system learns the examples by heart, then generalizes to new cases using a similarity measure to compare them to the learned examples or a subset of them.\n",
    "\n",
    "Instance-based learning algorithms are also known as memory-based learning or lazy learning.\n",
    "\n",
    "Examples of popular instance-based learning algorithms:\n",
    "\n",
    "- K-nearest neighbors (KNN)\n",
    "- Support vector machines (SVMs)\n",
    "- Gaussian processes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Model-based learning:` Model-based learning algorithms learn the underlying relationships and patterns in the data by creating a mathematical representation of the data. The model can then be used to make predictions on new data.\n",
    "\n",
    "Examples of popular model-based learning algorithms:\n",
    "\n",
    "- Linear regression\n",
    "- Logistic regression\n",
    "- Decision trees\n",
    "- Random forests\n",
    "- Neural networks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc4_'></a>[Main challenges of Machine Learning](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Insufficient quantity of training data.\n",
    "2. Non-representative training data.\n",
    "3. Poor quality data.\n",
    "4. Irrelevant features.\n",
    "5. Overfitting the training data (Usually overfitting happens when the model is too complex relative to\n",
    "the amount and noisiness of the training data).\n",
    "6. Underfitting the training data (This happens when the model is too simple to learn the underlying structure of the data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc5_'></a>[Model evaluation, Hyperparameter tuning and Model selection](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Hyperparameter:` A hyperparameter is a parameter of a learning algorithm (not of the model). The value of a hyperparameter has to be set before the training process begins. A hyperparameter controls the learning process and the model's capacity. \n",
    "\n",
    "`Regularization:` Regularization is a technique used to reduce overfitting by constraining a model’s complexity. For example, a simple regularization technique for linear models is to reduce the number of polynomial degrees. The amount of regularization to apply during learning can be controlled by a hyperparameter called the regularization hyperparameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Generalization error:` The generalization error is the error rate on new cases (i.e., cases that were not used for training the model). It is also known as ***out-of-sample error***."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Performance measurer:`  To know how good a model is we need to specify a performance measure. You can either define a ***utility function (or fitness function)*** that measures how good your model is, or you can define a ***cost function*** that measures how bad it is. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Loss function:` Often used synonymously with a ***cost function***. Sometimes the loss function is also called an ***error function***. In some literature, the term “loss” refers to the loss measured for a single data point, and the “cost” is a measurement that computes the loss (average or summed) over the entire dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <a id='toc5_1_'></a>[Testing and validating](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Training set:` The training set is the dataset used to train the model.\n",
    "\n",
    "`Test set:` The test set is the dataset used to test the model's performance on new data. This gives an estimate of the generalization error.\n",
    "\n",
    "`Validation set:` Once a particular learning algorithm have been selected we would want to fine-tune the hyperparameters to get the best possible model. Testing different candidate models (i.e, models with different hyperparameter values) on the same test set would result in the model that performs best on the test set, but this model may not perform well on new unseen data. So to avoid this, we use a ***validation set***. The validation set is used *to compare the performance of different candidate models and select the best performing model*. Once the best performing model is selected, it is retrained on the whole training set (training set + validation set) and the generalization error is measured on the test set. The validation set is also known as the ***dev set*** or ***development set***.\n",
    "\n",
    "`Repeated cross-validation:` If the validation set is too small, the model evaluation may be imprecise. If the validation set is too large, then the remaining training set will be too small and comparison of the candidate models will not be fair. To solve this problem, we can use ***repeated cross-validation***. Repeated cross-validation, uses many small validation sets. Each candidate model is evaluated once per validation set after it is trained on the rest of the data. By averaging out all the evaluations of a model, you get a much more accurate measure of its performance. There is a drawback, however: the training time is multiplied by the number of validation sets.\n",
    "\n",
    "`Data mismatch and the train-dev set:` Data mismatch is when the training data is different from the data used in production. This can happen due to many reasons, such as the data being sampled from different distributions, or the data being sampled at different times. This can result in the model performing poorly in production. To avoid this, it is important to use a validation set that is as close as possible to the data used in production.\n",
    "\n",
    "When real data is scarce, you may use similar abundant data for training and hold out some of it in a ***train-dev set*** to evaluate overfitting; the real data is then used to evaluate data mismatch (dev set) and to evaluate the final model’s performance (test set)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_intro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
